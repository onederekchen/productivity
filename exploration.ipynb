{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project scope\n",
    "\n",
    "Create a classifier that predicts how my day was based on inputs.\n",
    "\n",
    "Simplest possible pipeline just uses the productive and unproductive hours. \n",
    "\n",
    "Assumptions: the survey methodology was consistent throughout (which it mostly was). So I'm looking to predict how my day will be based on what I do during the day. It doesn't predict what I do during my day (I guess we could do that too, but it would be wildly inaccurate without a pretty insane number of features). \n",
    "\n",
    "The hours are a number from one through 8 (I was inconsistent with that, sometimes I would write higher numbers but really it should cap at 8). So there's a preprocessing step there.\n",
    "\n",
    "Note this is a pretty small amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data\n",
    "\n",
    "The data is from Sheets - I can even do my cleaning there if I want. That's probably better for the actual correction of individual pieces of data. But if there is actual preprocessing that needs to be done, it should be done in Python.\n",
    "\n",
    "The preprocessing step is just dropping NaNs - so if we had a month worth of new test data coming in in the same format, we would just drop any NaNs before running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"base data (dnc)/Daily Recap (Responses) - RAW form responses.csv\"\n",
    "target = 'How would you rate your overall satisfaction for the day?'\n",
    "\n",
    "df_full = pd.read_csv(path).dropna(subset=[target])  # ensure no missing y values\n",
    "\n",
    "train, test, y, y_test = train_test_split(df_full.drop(target, axis=1), df_full[target], random_state = 0)\n",
    "# train.columns\n",
    "# df_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that right now, I'm limiting all of the features up front. Later when I do feature selection, I'd probably limit the features here. Is there harm in doing that now instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Productive AI Hours', 'Productive Hours', 'Unproductive Hours']\n",
    "X = train[features].copy()\n",
    "X_test = test[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Productive AI Hours    0\n",
       "Productive Hours       0\n",
       "Unproductive Hours     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have an extra step to make sure X and y have the same number of features... that's because y doesn't usually have any NaNs. \n",
    "\n",
    "(!) duh, i'm struggling to find ways to drop the same X and y row because you usually don't drop rows. either drop the entire column, or you impute the value. The quick fix is for me to just add in the data though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess I could technically just test it on this test set for starters. That's pretty bad practice, but it should give me a rough idea of how predictive this is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=0)  # very basic model (?) plot change in performance vs training time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores (in %):\n",
      " [45.83333333 52.08333333 47.91666667 33.33333333 39.58333333]\n",
      "Average accuracy score (across experiments):\n",
      "43.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Accuracy scores (in %):\\n\", scores*100)\n",
    "print(\"Average accuracy score (across experiments):\")\n",
    "print(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's kind of low. But not that surprising give that I just gave it a bunch of numbers. Wait, does y need to be encoded?\n",
    "\n",
    "I guess let's get the scoring working and then we can see how much it improves it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I just want to be better than picking the same guess every time. Random would be 20% expected accuracy, I need to do exploratory analysis to determine what the most common value is (e.g. if I just guessed \"okay day\" every day). I hope it's under 50%. Oh shit, is this data imbalanced?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
